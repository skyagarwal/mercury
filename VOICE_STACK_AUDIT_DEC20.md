# üéØ Voice Stack Comprehensive Audit Report
**Date:** December 20, 2025  
**Mercury Server:** 192.168.0.151  
**Status:** ‚úÖ All Systems Operational

---

## üìä Executive Summary

| Component | Status | Details |
|-----------|--------|---------|
| **GPU Utilization** | ‚úÖ GOOD | 40% VRAM used, no duplicates |
| **TTS Service** | ‚úÖ HEALTHY | ChatterBox + Kokoro + ElevenLabs |
| **ASR Service** | ‚úÖ HEALTHY | Faster-Whisper on GPU |
| **Nerve/Exotel** | ‚úÖ RUNNING | 31 phrases pre-cached |
| **Hybrid TTS** | ‚úÖ CONFIGURED | Smart fallback enabled |
| **Latency** | ‚ö†Ô∏è NEEDS IMPROVEMENT | TTS ~3.3s, ASR ~470ms |

---

## üñ•Ô∏è GPU Status (RTX 3060 12GB)

### Current Allocation
```
+---------------------------------------------------------------------------------+
| Process                                          | VRAM Usage                   |
+---------------------------------------------------------------------------------+
| PID 1025559 (ASR - Faster Whisper)               | 898 MiB                      |
| PID 1073593 (TTS - ChatterBox)                   | 4016 MiB                     |
+---------------------------------------------------------------------------------+
| TOTAL                                            | 4922 MiB / 12288 MiB (40%)   |
+---------------------------------------------------------------------------------+
```

### CUDA Verification
```
‚úÖ CUDA Available: True
‚úÖ Device: NVIDIA GeForce RTX 3060
‚úÖ CUDA Version: 12.4
‚úÖ cuDNN Version: 90100
‚úÖ ChatterBox loads to GPU: ~3058 MB VRAM
```

### No Duplicate Processes
- ‚úÖ Only Docker containers running TTS/ASR
- ‚úÖ Single `voice_streaming_service.py` (PID 851204) outside Docker
- ‚úÖ No rogue Python processes consuming GPU

---

## üîä TTS Service (Port 7002)

### Providers Available
| Provider | Status | Use Case | Latency |
|----------|--------|----------|---------|
| **ChatterBox** | ‚úÖ Active | Hindi/Marathi (primary) | ~2.3-4.3s |
| **Kokoro** | ‚úÖ Active | English (ultra-fast) | ~50-100ms |
| **ElevenLabs** | ‚úÖ Active | Cloud fallback | ~500ms |
| **Deepgram** | ‚úÖ Active | Cloud fallback | ~400ms |

### Chotu Character Settings
```yaml
Default Language: Hindi (hi)
Emotions: sweet, innocent, helpful, polite, warm, shy, eager, neutral, happy, calm, professional
Styles: chotu, chotu_greeting, chotu_helpful, chotu_apologetic, conversational, narration
Voice Params:
  - exaggeration: 0.35 (gentle, not aggressive)
  - cfg_weight: 0.35 (slower, thoughtful pacing)
```

### TTS Latency Benchmarks (Hindi)
```
Run 1: 3394ms
Run 2: 2321ms  
Run 3: 4298ms
Average: ~3.3 seconds

Output: WAVE audio, 16-bit, mono, 24kHz
```

### Hybrid/Smart Fallback (‚úÖ IMPLEMENTED)
```python
# File: /app/providers/smart_fallback.py

Strategy:
1. Start ChatterBox (local GPU) 
2. After 500ms delay, start ElevenLabs (cloud)
3. Return whichever completes first
4. Cancel the other

Timeout: 3.0 seconds (configurable via INDIC_PARLER_TIMEOUT)
Mode: Hybrid (auto-select based on latency)
```

---

## üé§ ASR Service (Port 7001)

### Providers
| Provider | Status | Use Case |
|----------|--------|----------|
| **Faster-Whisper** | ‚úÖ Active | Primary (GPU-accelerated) |
| **Indic Conformer** | ‚úÖ Available | Indic languages backup |
| **Cloud Hybrid** | ‚úÖ Available | Deepgram/Google fallback |

### ASR Latency Benchmark
```
Input: 5.12 second Hindi audio
Processing Time: 469ms
Confidence: 1.0
Provider: faster-whisper
Transcription: "Namaste, I'm Chowotu."
```

**Note:** Transcription romanizes Hindi text. Consider using Indic Conformer for native Devanagari output.

---

## üìû Exotel/Nerve System (Port 7100)

### Status
```json
{
  "status": "healthy",
  "service": "nerve-system",
  "active_calls": 0,
  "tts_cache_size": 31,
  "components": {
    "tts_cache": true,
    "exotel_client": true,
    "jupiter_reporter": true
  }
}
```

### Pre-Cached TTS Phrases
- 31 common Hindi phrases pre-generated
- Reduces first-call latency by ~200ms
- Auto-preloaded on service startup

### Exotel Configuration
```
Exotel SID: sarvinsuppliesllp1
Caller ID: 02048556923
Callback URL: https://exotel.mangwale.ai/api/nerve/callback
```

### IVR Flow Working
```
1. POST /api/nerve/vendor-order-confirmation
   ‚Üí Exotel calls vendor
2. GET /api/nerve/callback (initial)
   ‚Üí Return Hindi greeting ExoML
3. Vendor presses 1/2
4. GET /api/nerve/callback?digits=1
   ‚Üí Return confirmation ExoML
5. Call ends with recording
```

### Jupiter AI Integration
```python
USE_JUPITER_AI = False  # Currently disabled (static scripts)
JUPITER_AI_URL = "http://192.168.0.156:3200"
```

**Recommendation:** Enable `USE_JUPITER_AI=true` to leverage FlowEngine + NLU + LLM for intelligent voice responses.

---

## üîå ElevenLabs Integration

### Configuration
```
API Key: ‚úÖ Configured (ELEVENLABS_API_KEY)
Fallback Provider: elevenlabs
Cloud Fallback: Enabled (ENABLE_CLOUD_FALLBACK=true)
```

### Available Voices
| Language | Voice ID | Type |
|----------|----------|------|
| English | 21m00Tcm4TlvDq8ikWAM (Rachel) | Default |
| Hindi | IKne3meq5aSn9XLyUdCD | Multilingual |
| Marathi | IKne3meq5aSn9XLyUdCD | Multilingual |

### Usage Pattern
1. **Primary:** ChatterBox (local GPU, free)
2. **Fallback:** ElevenLabs (cloud, paid per character)
3. **Hybrid Mode:** Race both, use faster

### Cost Consideration
- ElevenLabs: ~$0.30 per 1000 characters
- ChatterBox: Free (GPU compute only)
- **Recommendation:** Keep ElevenLabs as fallback only for reliability

---

## üöÄ Optimization Opportunities

### 1. TTS Latency Improvement
**Current:** ~3.3s average  
**Target:** <2s for conversational feel

**Options:**
- [ ] Use Kokoro for English (already ~50ms)
- [ ] Pre-generate responses for common order amounts
- [ ] Implement sentence chunking (stream first sentence while generating rest)
- [ ] Consider GPU batch processing for concurrent calls

### 2. ChatterBox CUDA Optimization
```python
# Already configured in providers/chatterbox.py
device: "cuda"
load_english_model: False  # Saves VRAM
use_turbo_for_english: False

# Emotion presets tuned for Chotu character
"sweet": (0.35, 0.35),     # Gentle tone
"innocent": (0.3, 0.4),    # Soft delivery
"helpful": (0.45, 0.35),   # Eager to help
```

### 3. GPU Utilization Enhancement
**Current:** ~40% VRAM used  
**Opportunity:** Can handle 10+ concurrent calls

**Bottleneck:** CPU-bound operations between GPU inference
- [ ] Implement async pipeline (ASR ‚Üí AI ‚Üí TTS)
- [ ] Use torch.compile() for faster inference
- [ ] Enable CUDA graphs for repeated inference patterns

### 4. Phase 0 Integration (Pending)
Connect Nerve System to Jupiter AI for intelligent responses:

```python
# Currently in nerve_system.py (disabled):
USE_JUPITER_AI = os.getenv("USE_JUPITER_AI", "false")

# To enable:
USE_JUPITER_AI=true
JUPITER_AI_URL=http://192.168.0.156:3200
```

Benefits:
- FlowEngine YAML flows work for voice
- NLU intent classification
- LLM-powered contextual responses
- Same AI brain as WhatsApp/Telegram

---

## üìã Action Items

### Immediate (This Week)
1. [ ] Enable Jupiter AI integration (`USE_JUPITER_AI=true`)
2. [ ] Add WhatsApp voice message support (see Phase 0 guide)
3. [ ] Test end-to-end Exotel call with real vendor

### Short Term (Next 2 Weeks)
4. [ ] Implement sentence streaming for TTS
5. [ ] Add monitoring/metrics for voice calls
6. [ ] Create voice character selection in admin UI

### Medium Term (Month 1-2)
7. [ ] WebRTC integration for in-app calls
8. [ ] AI Voice Agent for automated confirmations
9. [ ] Voice quality training data collection

---

## üìÅ Key Files Reference

| File | Purpose |
|------|---------|
| `/home/ubuntu/mangwale-voice/escotel-stack/exotel-service/nerve_system.py` | Exotel IVR orchestrator |
| `/app/providers/chatterbox.py` (in Docker) | ChatterBox TTS provider |
| `/app/providers/smart_fallback.py` (in Docker) | Hybrid TTS routing |
| `/home/ubuntu/mangwale-voice/STRATEGIC_VOICE_ARCHITECTURE.md` | Architecture roadmap |
| `/home/ubuntu/mangwale-voice/PHASE_0_IMPLEMENTATION_GUIDE.md` | Jupiter integration guide |
| `/home/ubuntu/mangwale-voice/EXOTEL_WORKING_SUCCESS.md` | Exotel setup docs |

---

## üîç Verification Commands

```bash
# GPU status
nvidia-smi

# Service health
curl http://localhost:7001/health  # ASR
curl http://localhost:7002/health  # TTS
curl http://localhost:7100/health  # Nerve

# TTS test
curl -X POST http://localhost:7002/synthesize \
  -H "Content-Type: application/json" \
  -d '{"text": "‡§®‡§Æ‡§∏‡•ç‡§§‡•á", "language": "hi"}' \
  --output test.wav

# ASR test
curl -X POST http://localhost:7001/transcribe \
  -F "file=@test.wav" \
  -F "language=hi"

# Nerve active calls
curl http://localhost:7100/api/nerve/active-calls
```

---

**Audit Completed:** December 20, 2025  
**Next Review:** January 3, 2026
