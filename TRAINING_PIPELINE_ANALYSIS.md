# ğŸ“ MANGWALE VOICE - TRAINING & DATA PIPELINE ANALYSIS

**Date**: December 19, 2025  
**Analyzed Systems**: Mercury (Voice Processing), Jupiter (AI + Storage)  
**Status**: Active Production System

---

## ğŸ“‹ EXECUTIVE SUMMARY

### What I Found:
âœ… **MinIO Storage System** - Audio storage infrastructure discovered  
âœ… **Automatic Training Data Collection** - TTS audio automatically saved with transcripts  
âŒ **Label Studio NOT FOUND** - No annotation system deployed  
âŒ **Training Scripts NOT FOUND** - No model fine-tuning pipeline  
âŒ **Exotel Recording Integration** - Not connected to training system  

### Current State:
- Audio storage: **Working** (MinIO on Jupiter:9002)
- Training data collection: **Partial** (TTS only, not calls)
- Annotation workflow: **Missing**
- Model retraining: **Manual**

---

## ğŸ—„ï¸ DISCOVERED: MINIO STORAGE ARCHITECTURE

### Location & Configuration
```
Server: Jupiter (192.168.0.156:9002)
Credentials: admin / minio_strong_password
Public URL: https://storage.mangwale.ai
Protocol: S3-compatible
```

### Bucket Structure
```
voice-audio/          # Real-time IVR audio files
  â””â”€â”€ ivr/
      â”œâ”€â”€ hi/{hash}.wav
      â””â”€â”€ en/{hash}.wav

call-recordings/      # Exotel call recordings (audit/compliance)
  â””â”€â”€ recordings/
      â””â”€â”€ 2025/
          â””â”€â”€ 12/
              â””â”€â”€ 19/
                  â””â”€â”€ {call_sid}.wav

training-data/        # TTS training pairs (audio + transcript)
  â””â”€â”€ tts/
      â”œâ”€â”€ hi/{hash}.wav
      â”œâ”€â”€ hi/{hash}.txt
      â”œâ”€â”€ en/{hash}.wav
      â””â”€â”€ en/{hash}.txt
```

### Storage Implementation
**File**: `nerve_system.py` (lines 208-279)

```python
class MinioAudioStorage:
    def __init__(self):
        self.client = Minio(
            "192.168.0.156:9002",
            access_key="admin",
            secret_key="minio_strong_password",
            secure=False
        )
        self.audio_bucket = "voice-audio"
        self.recordings_bucket = "call-recordings"
        self.training_bucket = "training-data"
    
    def upload_training_data(self, audio_data: bytes, text: str, language: str):
        """Upload audio+text pair for model training"""
        content_hash = hashlib.md5(f"{text}:{language}".encode()).hexdigest()[:12]
        filename = f"tts/{language}/{content_hash}.wav"
        
        # Save audio
        self.upload_audio(audio_data, filename, self.training_bucket)
        
        # Save transcript
        transcript_filename = f"tts/{language}/{content_hash}.txt"
        self.client.put_object(
            self.training_bucket,
            transcript_filename,
            io.BytesIO(text.encode()),
            length=len(text.encode()),
            content_type="text/plain"
        )
```

---

## ğŸ”„ CURRENT TRAINING DATA FLOW

### 1. TTS Generation â†’ Auto-Save Training Data
```
User makes call
  â†“
TTS generates Hindi audio "à¤‘à¤°à¥à¤¡à¤° à¤•à¤¨à¥à¤«à¤°à¥à¤® à¤•à¤°à¥‡à¤‚"
  â†“
Audio saved to MinIO:
  - training-data/tts/hi/abc123def456.wav
  - training-data/tts/hi/abc123def456.txt
  â†“
âœ… Audio-text pair ready for training
  â†“
âŒ No annotation step
  â†“
âŒ No automatic model retraining
```

### 2. Call Recordings â†’ NOT CONNECTED
```
Exotel call completed
  â†“
Recording saved to: call-recordings/recordings/2025/12/19/{call_sid}.wav
  â†“
âŒ No transcription
  â†“
âŒ No quality labeling
  â†“
âŒ Not used for ASR/TTS training
```

**Problem**: Call recordings are stored for audit but NOT integrated into training pipeline.

---

## âŒ WHAT'S MISSING: LABEL STUDIO & ANNOTATION

### Expected Workflow (NOT IMPLEMENTED):
```
1. Audio File â†’ MinIO Storage âœ…
2. Automatic Transcription (ASR) âŒ
3. Label Studio Annotation âŒ
   - Fix transcription errors
   - Mark audio quality
   - Tag accents/dialects
   - Label emotions
4. Export Annotated Dataset âŒ
5. Model Fine-tuning Pipeline âŒ
6. A/B Testing New Models âŒ
```

### Label Studio Search Results:
- **Config files**: Not found
- **Docker compose**: Not found
- **Annotation tasks**: Not found
- **Integration code**: Not found

**Conclusion**: Label Studio is NOT deployed. All annotation is manual.

---

## ğŸ—ï¸ TRAINING DATA INVENTORY

### Current TTS Training Data
**Location**: `training-data/tts/`

**Structure**:
```
{language}/{content_hash}.wav  # Audio file
{language}/{content_hash}.txt  # Transcript
```

**Languages**: Hindi (hi), English (en), Marathi (mr)

**Generation Source**: 
- IVR prompts (vendor confirmations, rider assignments)
- Generated by Indic-Parler TTS + Kokoro TTS
- Automatically saved during every TTS call

**Quality**: Unknown (no annotation)

### Current Call Recordings
**Location**: `call-recordings/recordings/YYYY/MM/DD/`

**Structure**:
```
{call_sid}.wav  # Full call audio
```

**Metadata**: 
- Call SID
- Timestamp
- Duration
- âŒ No transcript
- âŒ No quality score

---

## ğŸ¯ EXOTEL CALL RECORDING INTEGRATION

### Current Implementation:
```python
def upload_recording(self, audio_data: bytes, call_sid: str, metadata: dict = None):
    """Upload call recording for audit/training purposes"""
    filename = f"recordings/{datetime.now().strftime('%Y/%m/%d')}/{call_sid}.wav"
    return self.upload_audio(audio_data, filename, self.recordings_bucket)
```

### What Happens:
1. âœ… Call completes on Exotel
2. âœ… Recording saved to MinIO
3. âŒ No transcription generated
4. âŒ No quality check
5. âŒ No training pipeline trigger

### What's Needed:
```
Call Recording
  â†“
Automatic ASR Transcription (Faster-Whisper)
  â†“
Quality Check (WER, audio clarity, SNR)
  â†“
Label Studio Task Creation
  â†“
Human Review (fix errors, validate)
  â†“
Training Dataset Export
  â†“
Model Fine-tuning Trigger
```

---

## ğŸ“Š TRAINING DATA REQUIREMENTS ANALYSIS

### For ASR Fine-tuning (Whisper/IndicConformer):
**Current Status**: âŒ Not collecting

**Need**:
- Audio files: `.wav` 16kHz mono
- Transcripts: Accurate Hindi/Marathi/Roman Hinglish
- Duration: 100+ hours per language
- Quality: Clean audio, accurate labels

**Available Datasets** (Public):
```
ai4bharat/Rasa           â†’ 995K samples, 9 Indian languages
ai4bharat/Rural_Women    â†’ 64K samples, Hindi/Bhojpuri
google/fleurs            â†’ 12hrs/lang, 102 languages
```

### For TTS Fine-tuning (Indic-Parler/Orpheus):
**Current Status**: âœ… Partially collecting (IVR prompts only)

**Need**:
- Audio-text pairs
- Multiple speakers (vendor, rider, customer accents)
- Emotional variations (urgent, calm, happy)
- Natural prosody

**Available Datasets** (Public):
```
ai4b-hf/GLOBE-annotated  â†’ 535hrs, 18 languages
IndicTTS                 â†’ 382hrs, 12 languages
LIMMITS                  â†’ 568hrs, 7 languages
```

### For NLU Fine-tuning (IndicBERT):
**Current Status**: âœ… Database has 1,395 samples

**Location**: Jupiter PostgreSQL â†’ `nlu_training_data` table

**Breakdown**:
- Auto-generated: 977 samples
- English: 252 samples
- Hindi: 163 samples
- Marathi: 3 samples

---

## ğŸ”§ RECOMMENDED TRAINING PIPELINE ARCHITECTURE

### Phase 1: Label Studio Deployment (Week 1)

**1.1 Install Label Studio**
```bash
# On Jupiter
docker run -d \
  --name label-studio \
  -p 8080:8080 \
  -v /data/label-studio:/label-studio/data \
  heartexlabs/label-studio:latest
```

**1.2 Create Annotation Templates**

**ASR Annotation** (audio â†’ transcript):
```xml
<View>
  <Audio name="audio" value="$audio"/>
  <TextArea name="transcription" toName="audio" 
            rows="5" editable="true" maxSubmissions="1"/>
  <Choices name="quality" toName="audio" choice="single">
    <Choice value="excellent"/>
    <Choice value="good"/>
    <Choice value="fair"/>
    <Choice value="poor"/>
  </Choices>
  <Choices name="accent" toName="audio" choice="single">
    <Choice value="standard"/>
    <Choice value="rural"/>
    <Choice value="urban"/>
    <Choice value="code-mixed"/>
  </Choices>
</View>
```

**TTS Annotation** (rate generated audio):
```xml
<View>
  <Audio name="audio" value="$audio"/>
  <Text name="text" value="$transcript"/>
  <Rating name="naturalness" toName="audio" maxRating="5"/>
  <Rating name="pronunciation" toName="audio" maxRating="5"/>
  <Rating name="emotion" toName="audio" maxRating="5"/>
</View>
```

**1.3 Integration Script**
```python
# label_studio_integration.py
from label_studio_sdk import Client

ls = Client(url='http://192.168.0.156:8080', api_key='YOUR_KEY')
project = ls.start_project(
    title='Mangwale Voice Training',
    label_config='''<View>...</View>'''
)

# Auto-import from MinIO
def sync_minio_to_label_studio():
    storage = MinioAudioStorage()
    recordings = storage.client.list_objects('call-recordings', recursive=True)
    
    for recording in recordings:
        # Get presigned URL
        url = storage.client.presigned_get_object('call-recordings', recording.object_name)
        
        # Transcribe with ASR
        transcript = await transcribe_audio(url)
        
        # Create Label Studio task
        project.import_tasks([{
            'data': {
                'audio': url,
                'text': transcript['text'],
                'language': transcript['language']
            }
        }])
```

### Phase 2: Automatic Transcription Pipeline (Week 2)

**2.1 Post-Call Processing Service**
```python
# call_processor.py
@app.post("/process-recording")
async def process_recording(call_sid: str):
    # 1. Get recording from MinIO
    storage = MinioAudioStorage()
    audio_url = f"{MINIO_PUBLIC_URL}/call-recordings/recordings/*/{call_sid}.wav"
    
    # 2. Transcribe with Faster-Whisper
    transcript = await transcribe(audio_url)
    
    # 3. Quality check
    quality_score = check_audio_quality(audio_data)
    wer = calculate_wer(transcript, reference)  # if available
    
    # 4. Save metadata
    db.save_recording_metadata(call_sid, {
        'transcript': transcript['text'],
        'language': transcript['language'],
        'confidence': transcript['confidence'],
        'quality_score': quality_score,
        'duration': transcript['duration']
    })
    
    # 5. If quality > threshold, add to Label Studio
    if quality_score > 0.7:
        label_studio.create_task(call_sid, audio_url, transcript)
```

**2.2 Webhook from Exotel**
```python
# Add to nerve_system.py
@app.post("/api/nerve/exotel/recording-webhook")
async def recording_webhook(request: Request):
    """Called by Exotel when recording is ready"""
    data = await request.json()
    call_sid = data['CallSid']
    recording_url = data['RecordingUrl']
    
    # Download and store
    audio_data = await download_recording(recording_url)
    minio_url = storage.upload_recording(audio_data, call_sid)
    
    # Trigger processing
    await process_recording(call_sid)
```

### Phase 3: Model Fine-tuning Pipeline (Week 3-4)

**3.1 Dataset Export Script**
```python
# export_training_data.py
def export_annotated_dataset(language='hi', min_quality=0.8):
    """Export Label Studio annotations â†’ HuggingFace dataset"""
    from datasets import Dataset
    
    # Get completed annotations
    tasks = label_studio.get_completed_tasks(project_id)
    
    data = []
    for task in tasks:
        if task['quality_score'] >= min_quality:
            audio_path = download_audio(task['audio_url'])
            transcript = task['transcription']['text']
            
            data.append({
                'audio': audio_path,
                'transcription': transcript,
                'language': language,
                'speaker_id': task.get('speaker_id'),
                'accent': task.get('accent'),
                'quality': task['quality_score']
            })
    
    # Create HuggingFace dataset
    dataset = Dataset.from_list(data)
    dataset.push_to_hub("mangwale/voice-training-{language}")
    
    return dataset
```

**3.2 ASR Fine-tuning Script**
```python
# finetune_asr.py
from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer

# Load base model
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v3-turbo")

# Load Mangwale training data
train_dataset = load_dataset("mangwale/voice-training-hi", split="train")
eval_dataset = load_dataset("mangwale/voice-training-hi", split="test")

# Training config
training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-mangwale-hi",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=5000,
    fp16=True,
    eval_strategy="steps",
    eval_steps=500,
    save_steps=500,
    logging_steps=100,
)

# Train
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()
model.save_pretrained("./whisper-mangwale-hi-final")
```

**3.3 TTS Fine-tuning Script**
```python
# finetune_tts.py
from parler_tts import ParlerTTSForConditionalGeneration

# Load Indic-Parler base
model = ParlerTTSForConditionalGeneration.from_pretrained("ai4bharat/indic-parler-tts")

# Load Mangwale TTS pairs
train_dataset = load_dataset("mangwale/tts-training-hi", split="train")

# Fine-tune on Mangwale voices
trainer = TTSTrainer(
    model=model,
    train_dataset=train_dataset,
    learning_rate=1e-5,
    num_epochs=20
)

trainer.train()
model.save_pretrained("./indic-parler-mangwale")
```

### Phase 4: Production Deployment (Week 5)

**4.1 Model Versioning**
```python
# model_registry.py
class ModelRegistry:
    def __init__(self):
        self.models = {
            'asr': {
                'v1.0': 'openai/whisper-large-v3',
                'v1.1': 'mangwale/whisper-mangwale-hi-2025-01',
                'current': 'v1.0'
            },
            'tts': {
                'v1.0': 'ai4bharat/indic-parler-tts',
                'v1.1': 'mangwale/indic-parler-mangwale-2025-01',
                'current': 'v1.0'
            }
        }
    
    def switch_model(self, model_type, version):
        """Hot-swap models without downtime"""
        self.models[model_type]['current'] = version
        logger.info(f"Switched {model_type} to {version}")
```

**4.2 A/B Testing Framework**
```python
# ab_testing.py
class ABTest:
    def get_model_for_call(self, call_id):
        """50/50 split between models"""
        if hash(call_id) % 2 == 0:
            return 'v1.0'  # Baseline
        else:
            return 'v1.1'  # New model
    
    def track_metrics(self, call_id, model_version, metrics):
        """Track: latency, WER, user satisfaction"""
        db.save_ab_test_result(call_id, model_version, metrics)
```

**4.3 Automated Retraining Trigger**
```python
# auto_retrain.py
@scheduler.scheduled_job('cron', day_of_week='sun', hour=2)
async def weekly_retrain():
    """Check if we have enough new data to retrain"""
    new_samples = db.count_annotations_since_last_train()
    
    if new_samples >= 1000:
        logger.info(f"Triggering retraining with {new_samples} new samples")
        
        # Export dataset
        dataset = export_annotated_dataset(min_quality=0.8)
        
        # Fine-tune
        new_model = finetune_asr(dataset)
        
        # Evaluate
        wer = evaluate_model(new_model)
        
        if wer < current_wer:
            deploy_model(new_model, 'v1.2')
            start_ab_test('v1.1', 'v1.2')
        else:
            logger.warning("New model performs worse, not deploying")
```

---

## ğŸ“Š TRAINING DATA QUALITY METRICS

### Must Track:
1. **Audio Quality**
   - SNR (Signal-to-Noise Ratio)
   - Sample rate consistency
   - Duration distribution

2. **Transcription Quality**
   - Word Error Rate (WER)
   - Character Error Rate (CER)
   - Language detection accuracy

3. **Coverage**
   - Unique speakers
   - Accent distribution
   - Topic/domain coverage

4. **Annotation Quality**
   - Inter-annotator agreement
   - Review time per sample
   - Rejection rate

---

## ğŸš¨ CRITICAL GAPS & IMMEDIATE ACTIONS

### ğŸ”´ CRITICAL (Do Immediately):
1. **Deploy Label Studio** - No annotation system exists
2. **Connect Exotel recordings** - Recordings not used for training
3. **Auto-transcribe calls** - No transcripts generated

### ğŸŸ¡ HIGH PRIORITY (Week 1-2):
4. **Create annotation workflows** - Define quality standards
5. **Build dataset export pipeline** - Get data ready for training
6. **Set up model registry** - Version control for models

### ğŸŸ¢ MEDIUM PRIORITY (Week 3-4):
7. **Implement A/B testing** - Compare model versions
8. **Create retraining pipeline** - Automate fine-tuning
9. **Add quality metrics** - Track WER, latency, satisfaction

---

## ğŸ’¡ STRATEGIC RECOMMENDATIONS

### 1. Label Studio Deployment Strategy
**Option A: Self-Hosted (Recommended)**
- Deploy on Jupiter with Docker
- Full control over data
- No external dependency
- Cost: $0

**Option B: Label Studio Cloud**
- Managed service
- Easier setup
- Cost: $39/user/month

**Recommendation**: Self-host on Jupiter. You already have MinIO, PostgreSQL, and Docker infrastructure.

### 2. Annotation Workflow
**Phase 1**: Auto-transcribe ALL call recordings
**Phase 2**: Human review only low-confidence transcripts
**Phase 3**: Quality scoring (1-5 stars)
**Phase 4**: Export to training dataset

**Efficiency Target**: 
- 1 hour of audio â†’ 15 minutes of annotation time
- 1 annotator â†’ 4 hours of audio per day
- 2 annotators â†’ 80 hours of data per week

### 3. Model Retraining Cadence
**Initial**: Monthly (building baseline)
**Steady State**: Quarterly (refinement)
**Trigger**: 1000+ new high-quality samples

### 4. Training Data Priorities
1. **Hindi** - Primary language (60% of calls)
2. **Roman Hinglish** - Code-mixing detection
3. **English** - Fallback language
4. **Marathi** - Regional expansion

---

## ğŸ“ EXOTEL INTEGRATION DEEP DIVE

### Current Recording Flow:
```
Customer calls â†’ Exotel
  â†“
IVR flow (DTMF input)
  â†“
Call ends
  â†“
Exotel saves recording
  â†“
âŒ Webhook NOT configured
  â†“
Recording URL expires after 30 days
```

### Required Recording Flow:
```
Customer calls â†’ Exotel
  â†“
IVR flow (DTMF input)
  â†“
Call ends
  â†“
âœ… Exotel webhook â†’ nerve_system.py /recording-webhook
  â†“
Download recording from Exotel
  â†“
Upload to MinIO (permanent storage)
  â†“
Trigger transcription pipeline
  â†“
Create Label Studio annotation task
  â†“
Human reviews transcript
  â†“
Export to training dataset
```

### Implementation:
```python
# Add to nerve_system.py

@app.post("/api/nerve/exotel/recording-webhook")
async def exotel_recording_webhook(request: Request):
    """
    Exotel calls this when recording is ready
    
    POST data:
    {
        "CallSid": "abc123",
        "RecordingUrl": "https://s3.exotel.com/...",
        "RecordingDuration": "120",
        "CallFrom": "+919876543210",
        "CallTo": "+912048556923",
        "Status": "completed"
    }
    """
    data = await request.json()
    call_sid = data['CallSid']
    recording_url = data['RecordingUrl']
    duration = int(data['RecordingDuration'])
    
    logger.info(f"ğŸ“ Exotel recording webhook: {call_sid}, duration: {duration}s")
    
    # 1. Download recording from Exotel
    async with httpx.AsyncClient() as client:
        response = await client.get(
            recording_url,
            auth=(EXOTEL_API_KEY, EXOTEL_API_TOKEN)
        )
        audio_data = response.content
    
    # 2. Upload to MinIO (permanent storage)
    storage = get_minio_storage()
    minio_url = storage.upload_recording(audio_data, call_sid, {
        'duration': duration,
        'from': data['CallFrom'],
        'to': data['CallTo'],
        'exotel_url': recording_url
    })
    
    # 3. Transcribe with ASR
    transcript = await transcribe_audio(audio_data)
    
    # 4. Quality check
    quality = check_audio_quality(audio_data)
    
    # 5. Save to database
    await db.save_call_recording({
        'call_sid': call_sid,
        'audio_url': minio_url,
        'transcript': transcript['text'],
        'language': transcript['language'],
        'confidence': transcript['confidence'],
        'quality_score': quality,
        'duration': duration
    })
    
    # 6. Create Label Studio task (if high quality)
    if quality > 0.7 and transcript['confidence'] < 0.9:
        await label_studio.create_task({
            'audio_url': minio_url,
            'transcript': transcript['text'],
            'language': transcript['language'],
            'call_sid': call_sid
        })
    
    return {"status": "success", "call_sid": call_sid}
```

### Exotel API Configuration:
```
POST https://api.exotel.com/v1/Accounts/{ACCOUNT_SID}/Calls/connect
Headers:
  Authorization: Basic {base64(KEY:TOKEN)}
Body:
  RecordingStatusCallback: https://mercury.mangwale.ai/api/nerve/exotel/recording-webhook
  RecordingStatusCallbackMethod: POST
```

---

## ğŸ¯ SUCCESS METRICS

### Training Pipeline Health:
- **Data Collection Rate**: 10+ hours/week target
- **Annotation Throughput**: 80% of recordings reviewed within 7 days
- **Dataset Growth**: 100+ hours within 3 months
- **Model Improvement**: 10% WER reduction per quarter

### Model Performance:
- **ASR WER**: <15% (currently ~20%)
- **TTS MOS**: >4.0/5.0 (Mean Opinion Score)
- **Latency**: <300ms per component
- **System Uptime**: >99.5%

### Business Impact:
- **Call Success Rate**: >90%
- **User Satisfaction**: >4.2/5.0
- **Automation Rate**: >80% (no human intervention)

---

## ğŸ“ APPENDIX: FILE LOCATIONS

### Storage Infrastructure:
- **MinIO Client**: `nerve_system.py` lines 208-279
- **Training Data Upload**: `nerve_system.py` line 260-279
- **Call Recording Upload**: `nerve_system.py` line 256-258
- **Audio Buckets**: voice-audio, call-recordings, training-data

### TTS Training Data:
- **Auto-save Logic**: `nerve_system.py` line 1003
- **Storage Path**: `training-data/tts/{language}/{hash}.wav`
- **Transcript Path**: `training-data/tts/{language}/{hash}.txt`

### NLU Training Data:
- **Database**: Jupiter PostgreSQL â†’ `nlu_training_data` table
- **Samples**: 1,395 total (977 auto, 252 en, 163 hi, 3 mr)
- **Admin UI**: /home/ubuntu/mangwale-admin-frontend/src/pages/NLU.tsx

### Hybrid TTS Implementation:
- **Smart Fallback**: `mangwale-voice-v2/tts/providers/smart_fallback.py`
- **Parallel Racing**: Lines 220-280
- **Strategy**: Start both local+cloud, use faster result

---

## â“ CRITICAL QUESTIONS FOR USER

### Label Studio Integration:
1. **Do you want Label Studio self-hosted or cloud?**
   - Self-hosted: Free, full control, requires setup
   - Cloud: $39/user/month, easier, less control

2. **Who will do annotations?**
   - Internal team?
   - External contractors?
   - How many annotators?

3. **What annotation priority?**
   - ASR (call transcription) first?
   - TTS (quality rating) first?
   - Both parallel?

### Training Pipeline:
4. **Model retraining frequency?**
   - Weekly (aggressive)?
   - Monthly (steady)?
   - Quarterly (conservative)?

5. **A/B testing tolerance?**
   - 10% of traffic to new models?
   - 50/50 split?
   - Shadow mode (log only, don't use)?

6. **Training data quality bar?**
   - Accept all recordings?
   - Manual review required?
   - Quality score threshold (e.g., >0.8)?

### Exotel Integration:
7. **Current Exotel recording settings?**
   - Are recordings enabled?
   - What's the retention period?
   - Do you have webhook access?

8. **Storage requirements?**
   - How long to keep recordings?
   - Legal/compliance requirements?
   - Disk space budget?

### Resource Allocation:
9. **Training compute?**
   - Can we use Mercury GPU for training?
   - Need separate training server?
   - Cloud GPU budget (AWS/GCP)?

10. **Team bandwidth?**
    - Who maintains training pipeline?
    - Who reviews annotations?
    - Who monitors model performance?

---

## ğŸš€ NEXT STEPS (RECOMMENDED SEQUENCE)

### Week 1: Foundation
1. âœ… Deploy Label Studio on Jupiter (Docker)
2. âœ… Create annotation templates (ASR + TTS)
3. âœ… Configure Exotel recording webhook
4. âœ… Test recording â†’ MinIO â†’ Label Studio flow

### Week 2: Automation
5. âœ… Implement auto-transcription service
6. âœ… Build quality scoring pipeline
7. âœ… Create dataset export script
8. âœ… Test full pipeline end-to-end

### Week 3-4: Training
9. âœ… Collect 100+ hours of annotated data
10. âœ… Fine-tune Whisper on Mangwale Hindi
11. âœ… Fine-tune Indic-Parler on TTS data
12. âœ… Evaluate models (WER, MOS, latency)

### Week 5: Production
13. âœ… Deploy model registry
14. âœ… Implement A/B testing framework
15. âœ… Set up monitoring dashboards
16. âœ… Create weekly retraining cron job

---

**Status**: Ready for implementation  
**Blocker**: Label Studio not deployed, Exotel webhook not configured  
**Impact**: Currently NOT leveraging call data for model improvement  
**Urgency**: HIGH - Missing valuable training data every day
